{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:11:53.603467Z",
     "start_time": "2018-09-11T10:11:52.666802Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import random\n",
    "import tqdm\n",
    "import gensim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:11:53.613307Z",
     "start_time": "2018-09-11T10:11:53.605823Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('luxun.txt') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "corpus = corpus.replace('\\n', ' ').replace('\\r', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:11:53.769114Z",
     "start_time": "2018-09-11T10:11:53.616245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3759"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:11:53.979760Z",
     "start_time": "2018-09-11T10:11:53.771107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: 『鲁迅杂文经典全集（全本）/作者:鲁迅』\n",
      "indices: [1251, 3538, 1582, 2159, 2215, 1707, 743, 1195, 3707, 2999, 1195, 3532, 1133, 1588, 2715, 1131, 231, 3538, 1582, 246]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:11:54.042614Z",
     "start_time": "2018-09-11T10:11:53.982253Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_iter_random(corpus_indices, batch_size, num_steps):\n",
    "    # 减一是因为输出的索引是相应输入的索引加一。\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "    # 返回从 pos 开始的长为 num_steps 的序列\n",
    "    _data = lambda pos: corpus_indices[pos: pos + num_steps]\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取 batch_size 个随机样本。\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        X = [_data(j * num_steps) for j in batch_indices]\n",
    "        Y = [_data(j * num_steps + 1) for j in batch_indices]\n",
    "        yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:11:54.180778Z",
     "start_time": "2018-09-11T10:11:54.044997Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps):\n",
    "    corpus_indices = torch.tensor(corpus_indices)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0: batch_size*batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:11:54.524218Z",
     "start_time": "2018-09-11T10:11:54.183745Z"
    }
   },
   "outputs": [],
   "source": [
    "weight = torch.nn.Embedding(vocab_size, 400).weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:12:00.974356Z",
     "start_time": "2018-09-11T10:11:54.528454Z"
    }
   },
   "outputs": [],
   "source": [
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format('zh_text_char.vector',\n",
    "                                                          binary=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:12:01.011109Z",
     "start_time": "2018-09-11T10:12:00.981027Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(wvmodel.index2word)):\n",
    "    try:\n",
    "        index = word_to_idx[wvmodel.index2word[i]]\n",
    "    except:\n",
    "        continue\n",
    "    weight[index, :] = torch.from_numpy(wvmodel.get_vector(\n",
    "        idx_to_word[word_to_idx[wvmodel.index2word[i]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:12:01.098913Z",
     "start_time": "2018-09-11T10:12:01.014080Z"
    }
   },
   "outputs": [],
   "source": [
    "class lyricNet(nn.Module):\n",
    "    def __init__(self, hidden_dim, embed_dim, num_layers, weight,\n",
    "                 num_labels, bidirectional, dropout=0, **kwargs):\n",
    "        super(lyricNet, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_labels = num_labels\n",
    "        self.bidirectional = bidirectional\n",
    "        if num_layers <= 1 and dropout != 0:\n",
    "            self.dropout = 0\n",
    "        else:\n",
    "            self.dropout = dropout\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "#         self.gru = nn.GRU(input_size=self.embed_dim, hidden_size=self.hidden_dim,\n",
    "#                           num_layers=self.num_layers, bidirectional=self.bidirectional,\n",
    "#                           dropout=self.dropout)\n",
    "        self.lstm = nn.LSTM(input_size=self.embed_dim, hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.num_layers, bidirectional=self.bidirectional,\n",
    "                            dropout=self.dropout)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(hidden_dim * 2, self.num_labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(hidden_dim, self.num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, _ = self.lstm(embeddings.permute([1, 0, 2]))\n",
    "#         encoding = states[-1]\n",
    "        outputs = self.decoder(states.reshape((-1, states.shape[-1])))\n",
    "        return(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:12:01.232922Z",
     "start_time": "2018-09-11T10:12:01.101803Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 400\n",
    "hidden_dim = 100\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "num_epoch = 100\n",
    "use_gpu = True\n",
    "num_layers = 4\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "batch_size = 128\n",
    "device = torch.device('cuda:1')\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:12:03.902928Z",
     "start_time": "2018-09-11T10:12:01.236233Z"
    }
   },
   "outputs": [],
   "source": [
    "model = lyricNet(hidden_dim=hidden_dim, embed_dim=embedding_dim, num_layers=num_layers,\n",
    "                 num_labels=vocab_size, weight=weight, bidirectional=bidirectional,\n",
    "                 dropout=dropout)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "if use_gpu:\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:12:03.910523Z",
     "start_time": "2018-09-11T10:12:03.906031Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_acc(y_pred, y_true):\n",
    "    return int(sum(y_pred.argmax(dim=1) == y_true.t().reshape((-1,))).cpu()) / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:20:10.087105Z",
     "start_time": "2018-09-11T10:12:03.913311Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100, loss 8.2254, acc 0.0003, time 7.1786\n",
      "epoch 2/100, loss 8.1978, acc 0.0011, time 5.2194\n",
      "epoch 3/100, loss 8.1503, acc 0.0026, time 4.6997\n",
      "epoch 4/100, loss 8.0851, acc 0.0016, time 4.7651\n",
      "epoch 5/100, loss 7.9975, acc 0.0017, time 4.8398\n",
      "epoch 6/100, loss 7.8700, acc 0.0025, time 4.7612\n",
      "epoch 7/100, loss 7.6717, acc 0.0023, time 4.8045\n",
      "epoch 8/100, loss 7.3915, acc 0.0023, time 4.7217\n",
      "epoch 9/100, loss 7.2782, acc 0.0020, time 4.6693\n",
      "epoch 10/100, loss 7.1087, acc 0.0021, time 4.8869\n",
      "epoch 11/100, loss 7.0237, acc 0.0026, time 4.7287\n",
      "epoch 12/100, loss 6.9119, acc 0.0023, time 4.8144\n",
      "epoch 13/100, loss 6.7787, acc 0.0017, time 4.7893\n",
      "epoch 14/100, loss 6.7043, acc 0.0017, time 4.8017\n",
      "epoch 15/100, loss 6.6180, acc 0.0016, time 4.9248\n",
      "epoch 16/100, loss 6.5457, acc 0.0020, time 4.7953\n",
      "epoch 17/100, loss 6.4659, acc 0.0025, time 4.7910\n",
      "epoch 18/100, loss 6.3909, acc 0.0026, time 4.7207\n",
      "epoch 19/100, loss 6.3326, acc 0.0016, time 4.8124\n",
      "epoch 20/100, loss 6.2608, acc 0.0011, time 4.8555\n",
      "epoch 21/100, loss 6.1911, acc 0.0018, time 4.7853\n",
      "epoch 22/100, loss 6.1353, acc 0.0022, time 4.7790\n",
      "epoch 23/100, loss 6.0924, acc 0.0029, time 4.7581\n",
      "epoch 24/100, loss 6.0647, acc 0.0022, time 4.8260\n",
      "epoch 25/100, loss 6.0385, acc 0.0017, time 4.7654\n",
      "epoch 26/100, loss 6.0095, acc 0.0025, time 4.7506\n",
      "epoch 27/100, loss 5.9890, acc 0.0018, time 4.7681\n",
      "epoch 28/100, loss 5.9734, acc 0.0021, time 4.7581\n",
      "epoch 29/100, loss 5.9610, acc 0.0025, time 4.7839\n",
      "epoch 30/100, loss 5.9421, acc 0.0022, time 4.6825\n",
      "epoch 31/100, loss 5.9291, acc 0.0015, time 4.8530\n",
      "epoch 32/100, loss 5.9207, acc 0.0022, time 4.7721\n",
      "epoch 33/100, loss 5.9127, acc 0.0024, time 4.8854\n",
      "epoch 34/100, loss 5.8953, acc 0.0018, time 4.7507\n",
      "epoch 35/100, loss 5.8882, acc 0.0016, time 4.7746\n",
      "epoch 36/100, loss 5.8830, acc 0.0011, time 4.7544\n",
      "epoch 37/100, loss 5.8717, acc 0.0018, time 4.6904\n",
      "epoch 38/100, loss 5.8638, acc 0.0012, time 4.9085\n",
      "epoch 39/100, loss 5.8517, acc 0.0014, time 4.7460\n",
      "epoch 40/100, loss 5.8382, acc 0.0017, time 4.7792\n",
      "epoch 41/100, loss 5.8267, acc 0.0022, time 4.7444\n",
      "epoch 42/100, loss 5.8264, acc 0.0019, time 4.8532\n",
      "epoch 43/100, loss 5.8013, acc 0.0021, time 4.6573\n",
      "epoch 44/100, loss 5.7931, acc 0.0015, time 4.6759\n",
      "epoch 45/100, loss 5.7806, acc 0.0021, time 4.6484\n",
      "epoch 46/100, loss 5.7722, acc 0.0026, time 4.8863\n",
      "epoch 47/100, loss 5.7564, acc 0.0022, time 4.7418\n",
      "epoch 48/100, loss 5.7507, acc 0.0021, time 4.7510\n",
      "epoch 49/100, loss 5.7342, acc 0.0011, time 4.7888\n",
      "epoch 50/100, loss 5.7256, acc 0.0017, time 4.6869\n",
      "epoch 51/100, loss 5.7135, acc 0.0019, time 4.8000\n",
      "epoch 52/100, loss 5.7130, acc 0.0011, time 4.7555\n",
      "epoch 53/100, loss 5.6883, acc 0.0016, time 4.8935\n",
      "epoch 54/100, loss 5.6857, acc 0.0018, time 4.7441\n",
      "epoch 55/100, loss 5.6787, acc 0.0011, time 4.7559\n",
      "epoch 56/100, loss 5.6662, acc 0.0011, time 4.7880\n",
      "epoch 57/100, loss 5.6517, acc 0.0028, time 4.7974\n",
      "epoch 58/100, loss 5.6441, acc 0.0018, time 4.9497\n",
      "epoch 59/100, loss 5.6336, acc 0.0017, time 4.7136\n",
      "epoch 60/100, loss 5.6232, acc 0.0013, time 4.6588\n",
      "epoch 61/100, loss 5.6167, acc 0.0021, time 4.7219\n",
      "epoch 62/100, loss 5.6097, acc 0.0028, time 4.9290\n",
      "epoch 63/100, loss 5.5879, acc 0.0011, time 4.7659\n",
      "epoch 64/100, loss 5.5837, acc 0.0026, time 4.7971\n",
      "epoch 65/100, loss 5.5725, acc 0.0013, time 4.7687\n",
      "epoch 66/100, loss 5.5566, acc 0.0018, time 4.8027\n",
      "epoch 67/100, loss 5.5444, acc 0.0027, time 4.6078\n",
      "epoch 68/100, loss 5.5363, acc 0.0018, time 4.7848\n",
      "epoch 69/100, loss 5.5240, acc 0.0024, time 4.7433\n",
      "epoch 70/100, loss 5.5188, acc 0.0022, time 4.9500\n",
      "epoch 71/100, loss 5.5098, acc 0.0016, time 4.7123\n",
      "epoch 72/100, loss 5.4913, acc 0.0021, time 4.7630\n",
      "epoch 73/100, loss 5.4752, acc 0.0015, time 4.7776\n",
      "epoch 74/100, loss 5.4551, acc 0.0012, time 4.7361\n",
      "epoch 75/100, loss 5.4486, acc 0.0023, time 4.8194\n",
      "epoch 76/100, loss 5.4452, acc 0.0024, time 4.7131\n",
      "epoch 77/100, loss 5.4386, acc 0.0026, time 4.7133\n",
      "epoch 78/100, loss 5.4179, acc 0.0011, time 4.7395\n",
      "epoch 79/100, loss 5.4024, acc 0.0016, time 4.8257\n",
      "epoch 80/100, loss 5.4058, acc 0.0029, time 4.7751\n",
      "epoch 81/100, loss 5.3837, acc 0.0031, time 4.6707\n",
      "epoch 82/100, loss 5.3779, acc 0.0019, time 4.6227\n",
      "epoch 83/100, loss 5.3593, acc 0.0024, time 4.7596\n",
      "epoch 84/100, loss 5.3588, acc 0.0022, time 4.7209\n",
      "epoch 85/100, loss 5.3446, acc 0.0009, time 5.2078\n",
      "epoch 86/100, loss 5.3262, acc 0.0022, time 4.6504\n",
      "epoch 87/100, loss 5.3221, acc 0.0020, time 4.5262\n",
      "epoch 88/100, loss 5.3008, acc 0.0023, time 4.6464\n",
      "epoch 89/100, loss 5.2916, acc 0.0024, time 4.5038\n",
      "epoch 90/100, loss 5.2879, acc 0.0021, time 5.4154\n",
      "epoch 91/100, loss 5.2683, acc 0.0009, time 5.3691\n",
      "epoch 92/100, loss 5.2630, acc 0.0012, time 4.8077\n",
      "epoch 93/100, loss 5.2495, acc 0.0017, time 4.8003\n",
      "epoch 94/100, loss 5.2386, acc 0.0013, time 4.8657\n",
      "epoch 95/100, loss 5.2262, acc 0.0012, time 5.8241\n",
      "epoch 96/100, loss 5.2187, acc 0.0020, time 6.1910\n",
      "epoch 97/100, loss 5.1971, acc 0.0019, time 5.0752\n",
      "epoch 98/100, loss 5.1795, acc 0.0011, time 4.8318\n",
      "epoch 99/100, loss 5.1821, acc 0.0016, time 5.8394\n",
      "epoch 100/100, loss 5.1670, acc 0.0017, time 5.8439\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    start = time.time()\n",
    "    num, total_loss, total_acc = 0, 0, 0\n",
    "    data = data_iter_random(corpus_indices, batch_size, num_steps=500)\n",
    "#     if epoch == 50:\n",
    "#         optimizer.param_groups[0]['lr'] = lr * 0.1\n",
    "    for X, Y in data:\n",
    "        num += 1\n",
    "        if use_gpu:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(X)\n",
    "        l = loss_function(output, Y.t().reshape((-1,))).mean()\n",
    "        l.backward()\n",
    "        total_acc += eval_acc(output, Y)\n",
    "        optimizer.step()\n",
    "        total_loss += l.item()\n",
    "    end = time.time()\n",
    "    print('epoch %d/%d, loss %.4f, acc %.4f, time %.4f'\n",
    "          %(epoch+1, num_epoch, total_loss / num, total_acc / num, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:20:10.093813Z",
     "start_time": "2018-09-11T10:20:10.089178Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, model, device, idx_to_char, char_to_idx):\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix)):\n",
    "#         X = torch.tensor(output).to(device).reshape((1, len(output)))\n",
    "        X = torch.tensor([output[-1]]).to(device).reshape((1, 1))\n",
    "        pred = model(X)\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(pred.argmax(dim=1)[0]))\n",
    "    return(''.join([idx_to_char[i] for i in output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:20:10.380342Z",
     "start_time": "2018-09-11T10:20:10.095499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开的                                                  '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('分开', 50, model, device, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:20:10.395717Z",
     "start_time": "2018-09-11T10:20:10.382139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'其实，，，，，'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('其实', 4, model, device, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T10:20:10.542789Z",
     "start_time": "2018-09-11T10:20:10.397595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'悲剧将人生的有价值的东西毁灭给人看           '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('悲剧将人生的有价值的东西毁灭给人看', 10, model, device, idx_to_char, char_to_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
