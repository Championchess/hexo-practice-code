{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:44:48.568098Z",
     "start_time": "2018-08-10T10:44:47.248357Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "import time\n",
    "import snowballstemmer\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:44:48.579769Z",
     "start_time": "2018-08-10T10:44:48.570438Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    stemmed_words = [stemmer.stemWord(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:44:48.737635Z",
     "start_time": "2018-08-10T10:44:48.582804Z"
    }
   },
   "outputs": [],
   "source": [
    "# def readIMDB(path, seg='train'):\n",
    "#     pos_or_neg = ['pos', 'neg']\n",
    "#     data = []\n",
    "#     for label in pos_or_neg:\n",
    "#         files = os.listdir(os.path.join(path, seg, label))\n",
    "#         for file in files:\n",
    "#             with open(os.path.join(path, seg, label, file), 'r', encoding='utf8') as rf:\n",
    "#                 review = rf.read().replace('\\n', '')\n",
    "#                 if label == 'pos':\n",
    "#                     data.append([clean_text(review), 1])\n",
    "#                 elif label == 'neg':\n",
    "#                     data.append([clean_text(review), 0])\n",
    "#     return data\n",
    "\n",
    "def readIMDB(path, seg='train'):\n",
    "    pos_or_neg = ['pos', 'neg']\n",
    "    data = []\n",
    "    for label in pos_or_neg:\n",
    "        files = os.listdir(os.path.join(path, seg, label))\n",
    "        for file in files:\n",
    "            with open(os.path.join(path, seg, label, file), 'r', encoding='utf8') as rf:\n",
    "                review = rf.read().replace('\\n', '')\n",
    "                if label == 'pos':\n",
    "                    data.append([review, 1])\n",
    "                elif label == 'neg':\n",
    "                    data.append([review, 0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:44:50.795906Z",
     "start_time": "2018-08-10T10:44:48.740674Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = readIMDB('aclImdb')\n",
    "test_data = readIMDB('aclImdb', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:44:53.496322Z",
     "start_time": "2018-08-10T10:44:50.800401Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return [tok.lower() for tok in text.split(' ')]\n",
    "\n",
    "train_tokenized = []\n",
    "test_tokenized = []\n",
    "for review, score in train_data:\n",
    "    train_tokenized.append(tokenizer(review))\n",
    "for review, score in test_data:\n",
    "    test_tokenized.append(tokenizer(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:44:54.375360Z",
     "start_time": "2018-08-10T10:44:53.499805Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = set(chain(*train_tokenized))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:41.784585Z",
     "start_time": "2018-08-10T10:44:54.378275Z"
    }
   },
   "outputs": [],
   "source": [
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format('test_word.txt',\n",
    "                                                          binary=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:42.023498Z",
     "start_time": "2018-08-10T10:45:41.787586Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_idx = {word: i+1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "idx_to_word = {i+1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:42.034187Z",
     "start_time": "2018-08-10T10:45:42.027264Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:42.169054Z",
     "start_time": "2018-08-10T10:45:42.037132Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_samples(features, maxlen=500, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while(len(padded_feature) < maxlen):\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:49.113332Z",
     "start_time": "2018-08-10T10:45:42.172315Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = torch.tensor(pad_samples(encode_samples(train_tokenized, vocab)))\n",
    "train_labels = torch.tensor([score for _, score in train_data])\n",
    "test_features = torch.tensor(pad_samples(encode_samples(test_tokenized, vocab)))\n",
    "test_labels = torch.tensor([score for _, score in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:49.121610Z",
     "start_time": "2018-08-10T10:45:49.115695Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.use_gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "#         self.embedding.weight.data[1:(len(weight)+1), :] = weight\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "#         if self.bidirectional:\n",
    "#             self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "#         else:\n",
    "#             self.decoder = nn.Linear(num_hiddens * 1, labels)\n",
    "        self.decoder = nn.Linear(num_hiddens * 2, out_features=2)\n",
    "#         self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(\n",
    "            embeddings.view(inputs.shape[1], len(inputs), -1))\n",
    "        encoding = torch.cat([hidden[0][0], hidden[-1][0]], dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "#         encoding = F.dropout(encoding, training=self.training)\n",
    "#         outputs = self.decoder(hidden[-1][0])\n",
    "#         outputs = F.log_softmax(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:49.261805Z",
     "start_time": "2018-08-10T10:45:49.123395Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "embed_size = 100\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:51.443039Z",
     "start_time": "2018-08-10T10:45:49.264883Z"
    }
   },
   "outputs": [],
   "source": [
    "weight = nn.Embedding(vocab_size+1, embed_size).weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:52.997604Z",
     "start_time": "2018-08-10T10:45:51.446064Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(wvmodel.index2word)):\n",
    "    try:\n",
    "        index = word_to_idx[wvmodel.index2word[i]]\n",
    "    except:\n",
    "        continue\n",
    "    weight[index, :] = torch.tensor(wvmodel.get_vector(idx_to_word[word_to_idx[wvmodel.index2word[i]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:55.424075Z",
     "start_time": "2018-08-10T10:45:52.999763Z"
    }
   },
   "outputs": [],
   "source": [
    "net = SentimentNet(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional, weight=weight,\n",
    "                   labels=labels, use_gpu=True)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.8)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:55.429050Z",
     "start_time": "2018-08-10T10:45:55.425987Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_set = torch.utils.data.TensorDataset(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T10:45:55.578930Z",
     "start_time": "2018-08-10T10:45:55.432126Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-10T10:44:47.010Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 10.6311, train acc: 0.50, test loss: 29.6891, test acc: 0.50, time: 52.53\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, test_loss = 0, 0\n",
    "    train_acc, test_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    for feature, label in train_iter:\n",
    "        n += 1\n",
    "#         net.train()\n",
    "        net.zero_grad()\n",
    "        feature = Variable(feature.cuda())\n",
    "        label = Variable(label.cuda())\n",
    "        score = net(feature)\n",
    "        loss = loss_function(score, label)\n",
    "        loss.backward()\n",
    "#         scheduler.step()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                 dim=1), label.cpu())\n",
    "        train_loss += loss\n",
    "    with torch.no_grad():\n",
    "        for feature, label in test_iter:\n",
    "            m += 1\n",
    "#             net.eval()\n",
    "            feature = feature.cuda()\n",
    "            label = label.cuda()\n",
    "            score = net(feature)\n",
    "            loss = loss_function(score, label)\n",
    "            test_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                    dim=1), label.cpu())\n",
    "            test_loss += loss\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f' %\n",
    "          (epoch, train_loss.data / n, train_acc / n, test_loss.data / m, test_acc / m, runtime))\n",
    "#     print('epoch: %d, loss: %.4f, lr: %e'\n",
    "#           % (epoch, losses.data / n, scheduler.get_lr()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
