{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:42.290730Z",
     "start_time": "2018-10-18T07:48:41.959407Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:42.296307Z",
     "start_time": "2018-10-18T07:48:42.293358Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:42.374788Z",
     "start_time": "2018-10-18T07:48:42.299056Z"
    }
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    def __init__(self):\n",
    "        self.is_leaf = False\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.weight = None\n",
    "\n",
    "    def _calc_gain(self, G, H, G_l, G_r, H_l, H_r, lambd, gamma):\n",
    "        \"\"\"Measure how good a tree is. Equation 7\"\"\"\n",
    "        def calc_term(g, h):\n",
    "            return np.square(g) / (h + lambd)\n",
    "        gain = 0.5 * (calc_term(G_l, H_l) +\n",
    "                      calc_term(G_r, H_r) -\n",
    "                      calc_term(G, H)) - gamma\n",
    "        # the bigger gamma, the more convative\n",
    "        return gain\n",
    "\n",
    "    def _calc_leaf_weight(self, g, h, lambd):\n",
    "        \"\"\"Calc the optimal weight of leaf node. Equation 5\"\"\"\n",
    "        return np.sum(g) / (np.sum(h) + lambd)\n",
    "    \n",
    "    def build(self, instances, grad, hessian, eta, depth, param):\n",
    "        \"\"\"Algorithm 1\"\"\"\n",
    "        if depth > param['max_depth']:\n",
    "            # If the depth now is bigger than max depth, it is leaf node, and stop growing.\n",
    "            self.is_leaf = True\n",
    "            self.weight = self._calc_leaf_weight(grad, hessian, param['lambda']) * eta\n",
    "            return\n",
    "        G = np.sum(grad)\n",
    "        H = np.sum(hessian)\n",
    "        best_gain = 0.\n",
    "        best_feature = None\n",
    "        best_val = 0.\n",
    "        best_left_instances = None\n",
    "        best_right_instances = None\n",
    "        for feature in range(instances.shape[1]):\n",
    "            G_l, H_l = 0., 0.\n",
    "            sorted_instances = instances[:, feature].argsort()\n",
    "            for j in range(sorted_instances.shape[0]):\n",
    "                G_l += grad[sorted_instances[j]]\n",
    "                H_l += hessian[sorted_instances[j]]\n",
    "                G_r = G - G_l\n",
    "                H_r = H - H_l\n",
    "                current_gain = self._calc_gain(G, H, G_l, G_r, H_l, H_r,\n",
    "                                               param['lambda'], param['gamma'])\n",
    "                if current_gain > best_gain:\n",
    "                    best_gain = current_gain\n",
    "                    best_feature = feature\n",
    "                    best_val = instances[sorted_instances[j]][feature]\n",
    "                    best_left_instances = sorted_instances[:j+1]\n",
    "                    best_right_instances = sorted_instances[j+1:]\n",
    "        if best_gain < param['min_split_gain']:\n",
    "            self.is_leaf = True\n",
    "            self.weight = self._calc_leaf_weight(grad, hessian, param['lambda']) * eta\n",
    "        else:\n",
    "            self.split_feature = best_feature\n",
    "            self.split_value = best_val\n",
    "            self.left_child = TreeNode()\n",
    "            self.left_child.build(instances[best_left_instances],\n",
    "                                  grad[best_left_instances],\n",
    "                                  hessian[best_left_instances],\n",
    "                                  eta, depth+1, param)\n",
    "\n",
    "            self.right_child = TreeNode()\n",
    "            self.right_child.build(instances[best_right_instances],\n",
    "                                   grad[best_right_instances],\n",
    "                                   hessian[best_right_instances],\n",
    "                                   eta, depth+1, param)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.is_leaf:\n",
    "            return self.weight\n",
    "        else:\n",
    "            if x[self.split_feature] <= self.split_value:\n",
    "                return self.left_child.predict(x)\n",
    "            else:\n",
    "                return self.right_child.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:42.470160Z",
     "start_time": "2018-10-18T07:48:42.377235Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    \"\"\"Tree ensemble\"\"\"\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "    \n",
    "    def build(self, instances, grad, hessian, eta, param):\n",
    "        assert len(instances) == len(grad) == len(hessian)\n",
    "        self.root = TreeNode()\n",
    "        current_depth = 0\n",
    "        self.root.build(instances, grad, hessian, eta, current_depth, param)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.root.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:42.573279Z",
     "start_time": "2018-10-18T07:48:42.472753Z"
    }
   },
   "outputs": [],
   "source": [
    "class GBT(object):\n",
    "    def __init__(self):\n",
    "        self.params = {'gamma': 0.,\n",
    "                       'lambda': 1.,\n",
    "                       'min_split_gain': 0.1,\n",
    "                       'max_depth': 5,\n",
    "                       'learning_rate': 0.3}\n",
    "        self.best_iteration = None\n",
    "        \n",
    "    def _calc_training_data_scores(self, train_set, models):\n",
    "        if len(models) == 0:\n",
    "            return None\n",
    "        X = train_set.X\n",
    "        scores = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            scores[i] = self.predict(X[i], models=models)\n",
    "        return scores\n",
    "    \n",
    "    def _calc_l2_gradient(self, train_set, scores):\n",
    "        labels = train_set.y\n",
    "        hessian = np.full(len(labels), 2)\n",
    "        if scores is None:\n",
    "            grad = np.random.uniform(size=len(labels))\n",
    "        else:\n",
    "            grad = np.array([2 * (labels[i] - scores[i]) for i in range(len(labels))])\n",
    "        return grad, hessian\n",
    "    \n",
    "    def _calc_l2_loss(self, models, data_set):\n",
    "        errors = []\n",
    "        for x, y in zip(data_set.X, data_set.y):\n",
    "            errors.append(y - self.predict(x, models))\n",
    "        return np.mean(np.square(errors))\n",
    "    \n",
    "    def _build_learner(self, train_set, grad, hessian, eta):\n",
    "        learner = Tree()\n",
    "        learner.build(train_set.X, grad, hessian, eta, self.params)\n",
    "        return learner\n",
    "    \n",
    "    def train(self, params, train_set, valid_set=None, num_boost_rounds=20,\n",
    "              early_stopping_rounds=5, calc_grad=None, calc_loss=None):\n",
    "        self.params.update(params)\n",
    "        models = []\n",
    "        eta = self.params['learning_rate']\n",
    "        best_iteration = None\n",
    "        best_val_loss = np.infty\n",
    "        start = time.time()\n",
    "        \n",
    "        for cnt in range(num_boost_rounds):\n",
    "            iter_start = time.time()\n",
    "            scores = self._calc_training_data_scores(train_set, models)\n",
    "            if calc_grad is None:\n",
    "                grad, hessian = self._calc_l2_gradient(train_set, scores)\n",
    "            else:\n",
    "                grad, hessian = calc_grad(train_set, scores)\n",
    "            learner = self._build_learner(train_set, grad, hessian, eta)\n",
    "            models.append(learner)\n",
    "            if calc_loss is None:\n",
    "                train_loss = self._calc_l2_loss(models, train_set)\n",
    "            else:\n",
    "                train_loss = calc_loss(models, train_set)\n",
    "            if valid_set is not None:\n",
    "                if calc_loss is None:\n",
    "                    val_loss = self._calc_l2_loss(models, valid_set)\n",
    "                else:\n",
    "                    val_loss = calc_loss(models, valid_set)\n",
    "            else:\n",
    "                val_loss = None\n",
    "            if val_loss is not None and val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_iteration = cnt\n",
    "            val_loss_str = '{:.10f}'.format(val_loss) if val_loss else '-'\n",
    "            print(\"Iter {:>3}, Train's L2: {:.10f}, Valid's L2: {}, Elapsed: {:.2f} secs\"\n",
    "                  .format(cnt, train_loss, val_loss_str, time.time() - iter_start))\n",
    "            if cnt - best_iteration >= early_stopping_rounds:\n",
    "                print('Early stopping, best iteration is: %d' %(best_iteration))\n",
    "                break\n",
    "        self.models = models\n",
    "        self.best_iteration = best_iteration\n",
    "        print('Train finished. Elapsed: %.2fs, Train Loss: %.2f' %(time.time() - start, train_loss))\n",
    "        \n",
    "    def predict(self, x, models=None, num_iter=None):\n",
    "        if models is None:\n",
    "            models = self.models\n",
    "        assert models is not None\n",
    "        return np.sum([m.predict(x) for m in models[:num_iter]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:42.696770Z",
     "start_time": "2018-10-18T07:48:42.575674Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('pokemon.csv')\n",
    "data['Total'] = data['HP'] + data['Defense'] + data['Sp. Atk'] + data['Sp. Def'] + data['Speed'] + data['Attack']\n",
    "\n",
    "y_train = data.loc[:560, 'Attack'].values\n",
    "y_test = data.loc[560:, 'Attack'].values\n",
    "X_train = data.loc[:560, ['Total', 'HP', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']].values\n",
    "X_test = data.loc[560:, ['Total', 'HP', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']].values\n",
    "\n",
    "train_data = Dataset(X_train, y_train)\n",
    "eval_data = Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:50.670692Z",
     "start_time": "2018-10-18T07:48:42.699395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Iter   0, Train's L2: 7268.3460391774, Valid's L2: 7294.9877949834, Elapsed: 0.33 secs\n",
      "Iter   1, Train's L2: 3797.3869389644, Valid's L2: 3832.6476405716, Elapsed: 0.49 secs\n",
      "Iter   2, Train's L2: 2017.0013951855, Valid's L2: 2051.4683062482, Elapsed: 0.34 secs\n",
      "Iter   3, Train's L2: 1091.3516289110, Valid's L2: 1149.7871137416, Elapsed: 0.35 secs\n",
      "Iter   4, Train's L2: 622.7353858848, Valid's L2: 731.2915193225, Elapsed: 0.35 secs\n",
      "Iter   5, Train's L2: 376.8196404746, Valid's L2: 491.9337133165, Elapsed: 0.36 secs\n",
      "Iter   6, Train's L2: 245.5809016370, Valid's L2: 392.8329941207, Elapsed: 0.37 secs\n",
      "Iter   7, Train's L2: 171.5958983925, Valid's L2: 333.1821679251, Elapsed: 0.37 secs\n",
      "Iter   8, Train's L2: 129.7731004974, Valid's L2: 291.3854896863, Elapsed: 0.38 secs\n",
      "Iter   9, Train's L2: 104.1679395193, Valid's L2: 270.4561764188, Elapsed: 0.38 secs\n",
      "Iter  10, Train's L2: 90.0343849971, Valid's L2: 254.6067689354, Elapsed: 0.38 secs\n",
      "Iter  11, Train's L2: 78.1119468474, Valid's L2: 248.8626513523, Elapsed: 0.39 secs\n",
      "Iter  12, Train's L2: 71.3497548823, Valid's L2: 240.8314799815, Elapsed: 0.39 secs\n",
      "Iter  13, Train's L2: 63.3083653086, Valid's L2: 234.7785901805, Elapsed: 0.40 secs\n",
      "Iter  14, Train's L2: 58.5965523686, Valid's L2: 229.1240008032, Elapsed: 0.40 secs\n",
      "Iter  15, Train's L2: 55.1052310713, Valid's L2: 226.4496704383, Elapsed: 0.42 secs\n",
      "Iter  16, Train's L2: 51.9854440763, Valid's L2: 224.2844170952, Elapsed: 0.42 secs\n",
      "Iter  17, Train's L2: 48.2167575603, Valid's L2: 215.4513452152, Elapsed: 0.47 secs\n",
      "Iter  18, Train's L2: 44.0449919886, Valid's L2: 214.3286968235, Elapsed: 0.43 secs\n",
      "Iter  19, Train's L2: 41.2270099110, Valid's L2: 213.1866595353, Elapsed: 0.45 secs\n",
      "Train finished. Elapsed: 7.88s, Train Loss: 41.23\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "\n",
    "print('Start training...')\n",
    "gbt = GBT()\n",
    "gbt.train(params,\n",
    "          train_data,\n",
    "          valid_set=eval_data,\n",
    "          early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:50.702361Z",
     "start_time": "2018-10-18T07:48:50.673044Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for x in X_test:\n",
    "    y_pred.append(gbt.predict(x, num_iter=gbt.best_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:50.779285Z",
     "start_time": "2018-10-18T07:48:50.704778Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pd.Series(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:50.848232Z",
     "start_time": "2018-10-18T07:48:50.783348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean      78.790512\n",
       "std       24.662324\n",
       "min       27.365376\n",
       "25%       61.296114\n",
       "50%       74.252108\n",
       "75%       98.373989\n",
       "max      153.306740\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:51.624306Z",
     "start_time": "2018-10-18T07:48:50.850907Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T07:48:51.631824Z",
     "start_time": "2018-10-18T07:48:51.626345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.639969153776136"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred) ** 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
